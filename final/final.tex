\input{~/cheatsheet-preamble.tex}

	\\ AXLER
	\\ \textbf{ST=I iff TS=I (on vector spaces of the same dimension). } Suppose $V$ and $W$ are finite-dimensional vector spaces of the same dimension, $S \in \mathscr{L}(W,V), T \in \mathscr{L}(V,W)$. Then $ST=I$ iff $TS=I$. 
	\\ \textbf{matrix of identity operator with respect to two bases. } Suppose $u_1,\dots,u_n$ and $v_1,\dots,v_n$ are two bases of $V$. Then, the matrices $\mathscr{M}(I; u_1,\dots,u_n; v_1,\dots,v_n)$ and $\mathscr{M}(I; v_1,\dots,v_n; u_1,\dots,u_n)$ are invertible and are inverses of each other. 
	\\ \textbf{Change of basis formula. } Let $T \in \mathscr{L}(V,W)$. Suppose $u_1,\dots,u_n$ and $v_1,\dots,v_n$ are two bases of $V$. Let $A = \mathscr{M}(T; u_1,\dots,u_n)$ and $B = \mathscr{M}(T; v_1,\dots,v_n)$ and $C = \mathscr{M}(I; u_1,\dots,u_n; v_1,\dots,v_n)$. Then, $A = C^{-1}BC$. 
	\\ \textbf{eigenvalues are the zeros of minimal polynomial. } Let $V$ finite-dim and $T \in L(V)$. Then: 
	\begin{enumerate}
		\\ zeros of the minimal polynomial of $T$ are the eigenvalues of $T$. 
		\\ if $V$ is a complex vector space, then minimal polynomial of $T$ has the form $(z-\lambda_1) \cdot \dots \cdot (z-\lambda_m)$, where $\lambda_1,\dots,\lambda_m$ is a list of all eigenvalues of $T$, possibly with repetitions. 
	\end{enumerate}
	\\ \textbf{$q(T)=0$ iff $q$ is a polynomial multiple of the minimal polynomial. } Let $V$ finite-dim and $T \in L(V)$ and $q \in P(F)$. Then $q(T)=0$ iff $q$ is a polynomial multiple of the minimal polynomial. 
	\\ \textbf{minimal polynomial of a restriction operator. } Let $V$ finite-dim and $T \in L(V)$ and $U \subseteq V$ that is invariant under $T$. Then minimal polynomial of $T$ is a polynomial multiple of minimal polynomial of $T \mid_U$. 
	\\ \textbf{$T$ not invertible iff constant term of minimal polynomial of $T$ is 0. } Let $V$ finite-dim and $T \in L(V)$. Then $T$ is not invertible iff the constant term in the minimal polynomial of $T$ is 0. 
	\\ \textbf{even-dimensional null space. } Let $F = \mathbb{R}$ and $V$ finite-dim and $T \in L(V)$ and $b^2-4ac < 0$. Then $\dim(T^2 + bT + cI)$ is an even number. 
	\\ \textbf{operators on an odd-dimensional space have eigenvalues. } Every operator on an odd-dimensional vector space has an eigenvalue. 
	\\ \textbf{conditions for upper-triangular matrix. } Suppose $T \in L(V)$ and $v_1,\dots,v_n$ is a basis of $V$. Then TFAE: 
	\begin{enumerate}
		\\ the matrix of $T$ with respect to $v_1,\dots,v_n$ is upper-triangular. 
		\\ $\textrm{span}(v_1,\dots,v_k)$ is invariant under $T$ for each $k = 1,2,\dots,n$. 
		\\ $Tv_k \in \textrm{span}(v_1,\dots,v_k)$ for each $k=1,\dots,n$. 
	\end{enumerate}
	\\ \textbf{equation satisfied by operator with upper-triangular matrix. } Suppose $T \in L(V)$ and $V$ has a basis with respect to which $T$ has an upper-triangular matrix with diagonal entries $\lambda_1,\dots,\lambda_n$. Then $(T-\lambda_1 I) \cdot \dots \cdot (T-\lambda_n I) = 0$. 
	\\ \textbf{necessary and sufficient condition to have an upper-triangular matrix. } Suppose $V$ is finite-dim and $T \in L(V)$. Then $T$ has an upper-triangular matrix with respect to some basis of $V$ iff the minimal polynomial of $T$ equals $(z-\lambda_1) \cdot \dots \cdot (z-\lambda_n)$ for some $\lambda_i \in F$. 
	\\ \textbf{if $F = \mathbb{C}$, then every operator on $V$ has an upper-triangular matrix. } Suppose $V$ is a finite-dim complex vector space and $T \in L(V)$. Then $T$ has an upper-triangular matrix with respect to some basis of $V$. 
	\\ \textbf{conditions equivalent to diagonalizability. } Suppose $V$ finite-dim and $T \in L(V)$. Let $\lambda_1,\dots,\lambda_m$ denote the distinct eigenvalues of $T$. Then TFAE: 
	\begin{enumerate}
		\\ $T$ is diagonalizable. 
		\\ $V$ has a basis consisting of eigenvectors of $T$. 
		\\ $V = \oplus_i E(\lambda_i, T)$
		\\ $\dim V = \sum_i \dim E(\lambda_i,T)$. 
	\end{enumerate}
	\\ \textbf{enough eigenvalues implies diagonalizability. } Let $V$ be finite-dim and $T \in L(V)$ has $\dim V$ distinct eigenvalues. Then $T$ is diagonalizable. 
	\\ \textbf{necessary and sufficient condition for diagonalizability. } Suppose $V$ finite-dim and $T \in L(V)$. Then $T$ diagonalizable iff the minimal polynomial of $T$ equals $(z-\lambda_1) \cdot \cdot \cdot (z-\lambda_m)$ for some distinct $\lambda_1,\dots,\lambda_i \in F$. 
	\\ \textbf{restriction of diagonalizable operator to invariant subspace. } Suppose $T \in L(V)$ and $U$ is a $T$-invariant subspace of $V$. Then $T \mid_U$ is a diagonalizable operator on $U$. 
	\\ \textbf{commuting operators correspond to commuting matrices. } Suppose $S,T \in L(V)$ and $v_1,\dots,v_n$ is a basis of $V$. Then $S$ and $T$ commute iff $M(S,(v_1,\dots,v_n))$ and $M(T,(v_1,\dots,v_n))$ commute. 
	\\ \textbf{eigenspace is invariant under commuting operators. } Suppose $S,T \in L(V)$ commute and $\lambda \in F$. Then $E(\lambda,S)$ is invariant under $T$. 
	\\ \textbf{simultaneous diagonalizability iff commutativity. } Two diagonalizable operators on the same vector space have diagonal matrices with respect to the same basis iff the two operators commute. 
	\\ \textbf{common eigenvector for commuting operators. } every pair of commuting operators on a finite-dim nonzero complex vector space has a common eigenvector. 
	\\ \textbf{commuting operators are simultaneously upper-triangularizable. } Suppose $V$ is a finite-dim nonzero complex vector space and $S,T$ are commuting operators on $V$. Then there is a basis of $V$ with respect to which both $S,T$ have upper-triangular matrices. 
	\\ \textbf{eigenvalues of sum and product of commuting operators. } Suppose $V$ is a finite-dim complex vector space and $S,T$ are commuting operators on $V$. Then: 
	\begin{enumerate}
		\\ every eigenvalue of $S+T$ is an eigenvalue of $S$ plus an eigenvalue of $T$. 
		\\ every eigenvalue of $ST$ is an eigenvalue of $S$ times an eigenvalue of $T$. 
	\end{enumerate}
	\\ \textbf{$V$ is the direct sum of $\nul T^{\dim V}$ and $\range T^{\dim V}$. } Let $T \in L(V)$. Then $V = \nul T^{\dim V} \oplus \range T^{\dim V}$. 
	\\ \textbf{generalized eigenvector. } Let $T \in L(V)$ and $\lambda$ be an eigenvalue of $T$. A vector $v \in V$ ($v \neq 0$) is called a generalized eigenvector of $T$ corresponding to $\lambda$ if $(T - \lambda I)^k v = 0$ for some $k \in \mathbb{Z}_{>0}$. 
	\\ \textbf{generalized eigenvector corresponds to a unique eigenvalue. } Let $T \in L(V)$. Then each generalized eigenvector of $T$ corresponds to only one eigenvalue of $T$. 
	\\ \textbf{eigenvalues of nilpotent operator. } Let $T \in L(V)$. Then: 
	\begin{enumerate}
		\\ if $T$ is nilpotent then 0 is an eigenvalue of $T$ and $T$ has no other eigenvalues. 
		\\ if $F = \mathbb{C}$ and 0 is the only eigenvalue of $T$, then $T$ is nilpotent. 
	\end{enumerate}
	\\ \textbf{minimal polynomial \& upper-triangular matrix of nilpotent operator. } Let $T \in L(V)$. Then TFAE: 
	\begin{enumerate}
		\\ $T$ is nilpotent. 
		\\ minimal polynomial of $T$ is $z^m$ for some positive integer $m$. 
		\\ there is a basis of $V$ with respect to which the matrix of $T$ is upper-triangular with diagonal fully 0. 
	\end{enumerate}
	\\ \textbf{generalized eigenspace. } Suppose $T \in L(V)$ and $\lambda \in F$. The generalized eigenspace of $T$ corresponding to $\lambda$ is $G(\lambda, T) = \{v \in V \mid (T - \lambda I)^k \textrm{ for some } k \in \mathbb{Z}_{>0}\}$, which is the set of generalized eigenvectors of $T$ corresponding to $\lambda$, including the 0-vector. 
	\\ \textbf{description of generalized eigenspaces. } Suppose $T \in L(V)$ and $\lambda \in F$. Then $G(\lambda, T) = \nul(T - \lambda I)^{\dim V}$. 
	\\ \textbf{generalized eigenspace decomposition. } 
	\\ Suppose $F = \mathbb{C}$ and $T \in L(V)$. Let $\lambda_1,\dots,\lambda_m$ be the distinct eigenvalues of $T$. Then: 
	\begin{enumerate}
		\\ $G(\lambda_k,T)$ is invariant under $T$ for each $k=1,\dots,m$. 
		\\ $(T - \lambda_k I) \mid_{G(\lambda_k, T)}$ is nilpotent for each $k=1,\dots,m$. 
		\\ $V = \oplus_i G(\lambda_i, T)$. 
	\end{enumerate}
	\\ \textbf{multiplicity. } Let $T \in L(V)$. The multiplicity of an eigenvalue $\lambda$ of $T$ is defined to be the dimension of the corresponding generalized eigenspace $G(\lambda, T)$, so multiplicity of $\lambda$ is $\dim \nul(T - \lambda I)^{\dim V}$. 
	\\ \textbf{sum of the multiplicities equals $\dim V$. } Suppose $F=\mathbb{C}$ and $T \in L(V)$. Then the sum of all the multiplicities of all the eigenvalues of $T$ equals $\dim V$. 
	\\ \textbf{characteristic polynomial. } Let $F = \mathbb{C}$ and $T \in L(V)$. Let $\lambda_1,\dots,\lambda_m$ be the distinct eigenvalues of $T$, with multiplicities $d_1,\dots,d_m$. Then the polynomial $(z - \lambda_1)^{d_1} \cdot \cdot \cdot (z - \lambda_m)^{d_m}$ is called the characteristic polynomial of $T$. 
	\\ \textbf{degree and zeros of the characteristic polynomial. } Let $F = \mathbb{C}$ and $T \in L(V)$. Then: 
	\begin{enumerate}
		\\ characteristic polynomial of $T$ has degree $\dim V$. 
		\\ zeros of the characterisit polynomial are the eigenvalues of $T$. 
	\end{enumerate}
	\\ \textbf{Cayley-Hamilton theorem. } Let $F = \mathbb{C}$, $T \in L(V)$ and $q$ be the characteristic polynomial of $T$. Then $q(T) = 0$. 
	\\ \textbf{characteristic polynomial is a multiple of minimal polynomial. } Let $F = \mathbb{C}$ and $T \in L(V)$. Then characteristic polynomial of $T$ is a polynomial multiple of the minimal polynomial of $T$. 
	\\ \textbf{block diagonal matrix with upper-triangular blocks. } Let $F = \mathbb{C}$ and $T \in L(V)$. Let $\lambda_1,\dots,\lambda_m$ be the distinct eigenvalues of $T$ with multiplicities $d_1,\dots,d_m$. Then there is a basis of $V$ with respect to which $T$ has a block diagonal matrix of the form 
	$$
	\begin{pmatrix}
	A_1 & & 0 \\
	 & \ddots &  \\
	0 & & A_m
	\end{pmatrix}
	$$, where each $A_k$ is a $d_k$-by-$d_k$ upper-triangular matrix of the form 
	$$
	\begin{pmatrix}
	\lambda_k & & * \\
	 & \ddots &  \\
	0 & & \lambda_k
	\end{pmatrix}
	$$. 
	\\ \textbf{jordan basis. } Let $T \in L(V)$. A basis of $V$ is called a Jordan basis for $T$ if with respect to this basis $T$ has a block diagonal matrix 
	$$
	\begin{pmatrix}
	A_1 & & 0 \\
	 & \ddots & \\
	0 & & A_p
	\end{pmatrix}
	$$ in which each $A_k$ is an upper-triangular matrix of the form 
	$$
	\begin{pmatrix}
	\lambda_k & 1 & & 0 \\
	 & \ddots & \ddots & \\
	 & & \ddots & 1 \\
	0 & & & \lambda_k
	\end{pmatrix}
	$$. 
	\\ \textbf{every nilpotent operator has a jordan basis. } Let $T \in L(V)$ be nilpotent. Then there is a basis for $V$ that is a Jordan basis for $T$. 
	\\ \textbf{Jordan form. } Let $F = \mathbb{C}$ and $T \in L(V)$. Then there is a basis of $V$ that is a Jordan basis. 
	\\ \textbf{inner product. } An inner product on $V$ is a function that takes an ordered pair $(u,v)$ of elements of $V$ to a number $\langle u,v \rangle \in F$ so that 
	\begin{enumerate}
		\\ positivity: $\langle v,v \rangle \geq 0 \forall v \in V$. 
		\\ definiteness: $\langle v,v \rangle = 0$ iff $v = 0$. 
		\\ additivity in the first slot: $\langle u + v,w \rangle = \langle u,w \rangle + \langle v,w \rangle$. 
		\\ homogeneity in the first slot: $\langle \lambda u,v \rangle = \lambda \langle u,v \rangle$. 
		\\ conjugate symmetry: $\langle u,v \rangle = \overline{\langle v,u \rangle}$.
	\end{enumerate}
	\\ \textbf{pythagorean theorem. } if $u,v \in V$ with $u,v$ orthogonal, then $||u+v||^2 = ||u||^2 + ||v||^2$. 
	\\ \textbf{orthogonal decomposition. } let $u,v \in V$ with $v \neq 0$. set $c = \frac{\langle u,v \rangle}{||v||^2}$ and $w = u - \frac{\langle u,v \rangle}{||v||^2}$. then $u=cv+w$ and $\langle v,w \rangle = 0$. 
	\\ \textbf{cauchy-schwarz. } if $u,v \in V$, then $|\langle u,v \rangle| \leq ||u|| \cdot ||v||$, with equality iff one of $u,v$ is a scalar multiple of the other. 
	\\ \textbf{triangle inequality. } if $u,v \in V$, then $||u+v|| \leq ||u|| + ||v||$, with equality iff one of $u,v$ is a nonnegative real multiple of the other. 
	\\ \textbf{parallelogram equality. } if $u,v \in V$, then $||u+v||^2 + ||u-v||^2 = 2(||u||^2 + ||v||^2)$. 
	\\ \textbf{norm of an orthonormal linear combination. } let $e_1,\dots,e_m$ be an orthonormal list in $V$. then $||a_1e_1 + \dots + a_me_m||^2 = |a_1|^2 + \dots + |a_m|^2$. 
	\\ \textbf{bessel's inequality. } let $e_1,\dots,e_m$ be an orthonormal list in $V$. then if $v \in V$, then $|\langle v,e_1 \rangle|^2 + \dots + |\langle v,e_m \rangle|^2 \leq ||v||^2$. 
	\\ \textbf{writing a vector as a linear combination of an orthonormal basis. } let $e_1,\dots,e_m$ be an orthonormal basis of $V$ and $u,v \in V$. then 
	\begin{enumerate}
		\\ $v = \langle v,e_1 \rangle e_1 + \dots + \langle v,e_m \rangle e_m$. 
		\\ $||v||^2 = |\langle v,e_1 \rangle|^2 + \dots + |\langle v,e_m \rangle|^2$. 
		\\ $\langle u,v \rangle = \langle u,e_1 \rangle \overline{\langle v,e_1 \rangle} + \dots + \langle u,e_m \rangle \overline{\langle v,e_m \rangle}$. 
	\end{enumerate}
	\\ \textbf{upper-triangular matrix with respect to some orthonormal basis. } let $V$ be finitedim and $T \in L(V)$. then $T$ has an upper-triangular matrix with respect to some orthonormal basis of $V$ iff min poly of $T$ equals $(z-\lambda_1) \cdot \cdot \cdot (z-\lambda_m)$ for some $m$. 
	\\ \textbf{schur's theorem. } every operator on a finitedim complex inner product space has an upper-triangular matrix with respect to some orthonormal basis. 
	\\ \textbf{riesz representation theorem. } let $V$ be finitedim and $\phi$ is a linear functional on $V$. then there is a unique $v \in V$ so that $\phi(u) = \langle u,v \rangle$ for every $u \in V$. 
	\\ \textbf{adjoint. } let $T \in L(V,W)$. the adjoint of $T$ is $T^*: W \to V$ so that $\langle Tv,w \rangle = \langle v,T^*w \rangle$ for all $v \in V, w \in W$. 
	\\ \textbf{properties of adjoint. } Let $T \in L(V,W)$. 
	\begin{enumerate}
		\\ $(S+T)^* = S^*+T^*$. 
		\\ $(\lambda T)^* = \overline{\lambda}T^*$. 
		\\ $(T^*)^*=T$. 
		\\ $(ST)^* = T^*S^*$ for all $S \in L(W,U)$, where $U$ is a finitedim inner product space. 
		\\ $I^*=I$. 
		\\ if $T$ is invertible, then $T^*$ is invertible and $(T^*)^{-1} = (T^{-1})^*$.
	\end{enumerate}
	\\ \textbf{null space and range of $T^*$. } Let $T \in L(V,W)$. then 
	\begin{enumerate}
		\\ $\nul T^* = (\range T)^\perp$. 
		\\ $\range T^* = (\nul T)^\perp$. 
		\\ $\nul T = (\range T^*)^\perp$. 
		\\ $\range T = (\nul T^*)^\perp$. 
	\end{enumerate}
	\\ \textbf{eigenvalues of self-adjoint operators. } every eigenvalue of a self-adjoint operator is real. 
	\\ \textbf{prop. } Suppose $V$ is a complex inner product space and $T \in L(V)$. then $\langle Tv,v \rangle = 0 \forall v \in V$ iff $T=0$. 
	\\ \textbf{prop. } suppose $V$ is a complex inner product space and $T \in L(V)$. then $T$ is self-adjoint iff $\langle Tv,v \rangle \in R \forall v \in V$. 
	\\ \textbf{prop. } let $T$ be a self-adjoint operator on $V$. then $\langle Tv,v \rangle = 0 \forall v \in V$ iff $T=0$. 
	\\ \textbf{prop. } let $T \in L(V)$. then $T$ is normal iff $||Tv|| = ||T^*v|| \forall v \in V$. 
	\\ \textbf{range, null space, eigenvectors of a normal operator. } let $T \in L(V)$ be normal. then
	\begin{enumerate}
		\\ $\nul T = \nul T^*$. 
		\\ $\range T = \range T^*$. 
		\\ $V = \nul T \oplus \range T$. 
		\\ $T-\lambda I$ is normal for all $\lambda in F$. 
		\\ if $v \in V$ and $\lambda \in F$, then $Tv=\lambda v$ iff $T^*v = \overline{\lambda}v$. 
	\end{enumerate}
	\\ \textbf{$T$ normal iff real/imaginary parts of $T$ commute. } let $F=C$ and $T \in L(V)$. then $T$ is normal iff there exist commuting self-adjoint operators $A,B$ so that $T=A+iB$. 
	\\ \textbf{minimal polynomial of self-adjoint operator. } let $T \in L(V)$ self-adjoint. then the min poly of $T$ equals $(z-\lambda_1) \cdot \cdot \cdot (z-\lambda_m)$ for some $m$. 
	\\ \textbf{Real Spectral Theorem. } let $F=R$ and $T \in L(V)$. TFAE: 
	\begin{enumerate}
		\\ $T$ is self-adjoint. 
		\\ $T$ has a diagonal matrix with respect to some orthonormal basis of $V$. 
		\\ $V$ has an orthonormal basis consisting of eigenvectors of $T$. 
	\end{enumerate}
	\\ \textbf{Complex Spectral Theorem. } let $F=C$ and $T \in L(V)$. TFAE: 
	\begin{enumerate}
		\\ $T$ is normal. 
		\\ $T$ has a diagonal matrix with respect to some orthonormal basis of $V$. 
		\\ $V$ has an orthonormal basis consisting of eigenvectors of $T$. 
	\end{enumerate}
	\\ \textbf{positive operator. } an operator $T \in L(V)$ is called positive if it's self-adjoint and $\langle Tv,v \rangle \geq 0 \forall v \in V$. 
	\\ \textbf{characterizations of positive operators. } let $T \in L(V)$. TFAE: 
	\begin{enumerate}
		\\ $T$ is a positive operator. 
		\\ $T$ is self-adjoint and all eigenvalues of $T$ are nonnegative. 
		\\ with respect to some orthonormal basis of $T$, the matrix of $T$ is diagonal with only nonnegative numbers on diagonal. 
		\\ $T$ has a positive square root. 
		\\ $T$ has a self-adjoint square root. 
		\\ $T=R^*R$ for some $R \in L(V)$. 
	\end{enumerate}
	\\ \textbf{isometry. } a linear map $S \in L(V,W)$ is an isometry if $||Sv|| = ||v|| \forall v \in V$. 
	\\ \textbf{characterizations of isometries. } let $S \in L(V,W)$. let $e_1,\dots,e_n$ be an orthonormal basis of $V$ and $f_1,\dots,f_m$ be an orthonormal basis of $W$. TFAE: 
	\begin{enumerate}
		\\ $S$ is an isometry. 
		\\ $S^*S = = SS^* = I$. 
		\\ $\langle Su,Sv \rangle = \langle u,v \rangle \forall u,v \in V$. 
		\\ $Se_1,\dots,Se_n$ is an orthonormal list in $W$. 
		\\ the columns of $M(S,(e_1,\dots,e_n), (f_1,\dots,f_m))$ form an orthonormal list in $F^m$ with respect to the Euclidean inner product. 
	\end{enumerate}
	\\ \textbf{unitary operator. } An operator $S \in L(V)$ is called unitary if it is an invertible isometry. 
	\\ \textbf{characterizations of unitary operators. } let $S \in L(V)$ and $e_1,\dots,e_n$ be an orthonormal basis of $V$. TFAE: 
	\begin{enumerate}
		\\ $S$ is a unitary operator. 
		\\ $S^*S = SS^* = I$. 
		\\ $S$ is invertible and $S^{-1} = S^*$. 
		\\ $Se_1,\dots,Se_n$ is an orthonormal basis of $V$. 
		\\ the rows of $M(S,(e_1,\dots,e_n))$ form an orthonormal basis of $F^n$ with respect to the Euclidean inner product. 
		\\ $S^*$ is a unitary operator. 
	\end{enumerate}
	\\ \textbf{description of unitary operators on complex inner product spaces. } let $F=C$ and $S \in L(V)$. TFAE: 
	\begin{enumerate}
		\\ $S$ is a unitary operator. 
		\\ there is an orthonormal basis of $V$ consisting of eigenvectors of $S$ whose corresponding eigenvalues all have absolute value 1. 
	\end{enumerate}
	\\ \textbf{properties of $T^*T$. } Let $T \in L(V,W)$. then 
	\begin{enumerate} 
		\\ $T^*T$ is a positive operator on $V$. 
		\\ $\nul T^*T = \nul T$. 
		\\ $\range T^*T = \range T^*$. 
		\\ $\dim\range T = \dim\range T^* = \dim\range T^*T$. 
	\end{enumerate}
	\\ \textbf{singular values. } let $T \in L(V,W)$. the singular values of $T$ are the nonnegative square roots of eigenvalues of $T^*T$, listed in decreasing order, each included as many times as the dimension of the corresponding eigenspace of $T^*T$. 
	\\ \textbf{role of positive singular values. } let $T \in L(V,W)$. then 
	\begin{enumerate}
		\\ $T$ is 1-1 iff 0 is not a singular value of $T$. 
		\\ number of positive singular values of $T$ equals $\dim\range T$. 
		\\ $T$ is onto iff number of positive singular values of $T$ equals $\dim W$. 
	\end{enumerate}
	\\ \textbf{isometries characterized by having all singular values equal 1. } let $S \in L(V,W)$. then $S$ is an isometry iff all singular values of $S$ equal 1. 
	\\ \textbf{singular value decomposition. } let $T \in L(V,W)$ and positive singular values of $T$ are $s_1,\dots,s_m$. then there exist orthonormal lists $e_1,\dots,e_m \in V$ and $f_1,\dots,f_m \in W$ so that $Tv = s_1 \langle v,e_1 \rangle f_1 + \dots + s_m \langle v,e_m \rangle f_m$ for all $v \in V$. 
	\\ \textbf{matrix version of SVD. } let $A$ be a $p \times n$ matrix with $\rank A \geq 1$. then there exist a $p \times m$ matrix $B$ with orthonormal columns, an $m \times m$ matrix $D$ with positive numbers on diagonal, and an $n \times m$ matrix $C$ with orthonormal columns so that $A=BDC^*$. 
	\\ \textbf{upper bound for $||Tv||$. } let $T \in L(V,W)$. let $s_1$ be the largest singular value for $T$. then $||Tv|| \leq s_1||v|| \forall v \in V$.
	\\ \textbf{norm of a linear map $||\cdot||$. } let $T \in L(V,W)$. then define norm of $T$ has $||T|| = \max\{||Tv|| \mid v \in V, ||v|| \leq 1\}$. 
	\\ \textbf{alternative formulas for $||T||$. } let $T \in L(V,W)$. then
	\begin{enumerate}
		\\ $||T||$ is the largest singular value of $T$. 
		\\ $||T|| = \max\{||Tv|| \mid v \in V, ||v||=1\}$. 
		\\ $||T||$ is the smallest number $c$ so that $||Tv|| \leq c||v||$ for all $v \in V$. 
	\end{enumerate}
	\\ \textbf{norm of adjoint. } let $T \in L(V,W)$. then $||T^*|| = ||T||$. 
	\\ \textbf{best approximation by linear map whose range has dimension $\leq k$. } let $T \in L(V,W)$ and $s_1 \geq \dots \geq s_m$ are the positive singular values of $T$. let $1 \leq k \leq m$. then $\min\{||T-S|| \mid S \in L(V,W), \dim\range S \leq k\} = s_{k+1}$. Also, if $Tv = s_1 \langle v,e_1 \rangle f_1 + \dots + s_m \langle v,e_m \rangle f_m$ is a singular value decomposition of $T$ and $T_k \in L(V,W)$ is defined by $T_kv = s_1 \langle v,e_1 \rangle f_1 + \dots + s_k \langle v,e_k \rangle f_k$ for each $v \in V$, then $\dim\range T_k = k$ and $||T-T_k|| = s_{k+1}$. 
	\\ \textbf{polar decomposition. } let $T \in L(V)$. then there exists a unitary operator $S \in L(V)$ so that $T = S\sqrt{T^*T}$. 
	\\ RIBET DEFS. 
	\\ \textbf{Characteristic polynomial. } The characteristic polynomial of $T: V \to V$ (with eigenvalues $\lambda_1,\dots,\lambda_t$) is the polynomial $\prod_{i=1}^{t} (x-\lambda_i)^{\dim X_i}$, where $V = X_1 \oplus \dots \oplus X_t$. 
	\\ \textbf{Simultaneously diagonalizable. } Operators $S$ and $T$ on $V$ are simulatenously diagonalizable if there is a basis of $V$ that consts of vectors that are eigenvectors for both $S$ and $T$ (i.e. there exists a basis $v_1,\dots,v_n$ of $V$ so that for $i$, $1 \leq i \leq n$, there are $\lambda_i$ and $\mu_i$ so that $Sv_i = \lambda_iv_i$ and $Tv_i = \mu_iv_i$). 
	\\ \textbf{Complex Inner product. } Let $z=(z_1,\dots,z_n)$ and $w=(w_1,\dots,w_n)$ be in $\mathbb{C}^n$. We have $\langle z, w \rangle = \sum_{i} z_i \overline{w_i}$, with $\langle w, z \rangle = \overline{\langle z, w \rangle}$ and $\langle \alpha z, w \rangle = \alpha \langle z, w \rangle$ and $\langle w, \alpha z \rangle = \overline{\alpha}\langle z, w \rangle$. 
	\\ \textbf{Adjoint. } Let $T: V \to W$. Then the adjoint of $T$ is $T^*: W \to V$ such that $T^* = \alpha_V^{-1} \circ T' \circ \alpha_W$, where $\alpha_V: V \to V'$ and $\alpha_W: W \to W'$. 
	\\ \textbf{Def 1 of Determinants. } (cofactor expansion). $\det A = \sum_{j=1}^n (-1)^{j+1} \cdot a_{ij} \det (A_{ij})$. 
	\\ \textbf{Def 2 of Determinants. } If $T \in L(V)$, where $V$ is over $\mathbb{C}$, then the determinant of $T$ is the product of the eigenvalues $\lambda_1,\dots,\lambda_n$ (with multiplicity). 
	\\ \textbf{Def 3 of Determinants. } The determinant of an operator $T \in L(V)$ is $(-1)^n$ times the constant term of its characteristic polynomial $(z-\lambda_1) \cdot \cdot \cdot (z-\lambda_n)$. 
	\\ \textbf{Permutation. } A permutation $m=(m_1,\dots,m_r)$ is a list containing $1,\dots,n$ exactly once each, and write $S_n$ to denote the set of $n$-element permutations. 
	\\ \textbf{Inversion. } An inversion (in a permutation) is a pair $\{i,j\}$ such that $m_i > m_j$ where $i < j$. 
	\\ \textbf{Def 4 of Determinants. } Let $A$ be an $n \times n$ matrix. Then, $\det A = \sum_{m \in S_n} \left(\sgn(m) \cdot a_{m_1,1} \cdot \cdot \cdot a_{m_n,n}\right)$. 
	\\ \textbf{Box. } The box defined by $v_1,\dots,v_n \in \mathbb{R}^n$ is $\{a_1v_1 + \dots + a_nv_n \mid 0 \leq a_i \leq 1, i =1,\dots,n\}$ and as volume, $\det \begin{pmatrix} v_1 & \dots & v_n \end{pmatrix}$, where $v_1,\dots,v_n$ are the columns of a matrix $A$. 
	\\ \textbf{Norm of $T$. } This is defined to be $\sup_{v \in V \setminus \{0\}} \frac{||Tv||}{||v||}$. 
	\\ \textbf{Tensor product. } $V \otimes W := \Bil(V',W')$. 
	\\ RIBET THMS. 
	\\ \textbf{Cor. } The annihilator of $U$ is $\{0\}$ iff $U = V$. The annihilator of $U$ is $V$ iff $U = \{0\}$. 
	\\ \textbf{Prop. } If $T: V \to W$ is a linear map, then the null space of $T'$ is the annihilator of the range of $T$. We have $\textrm{ann}(\textrm{range}T) = \{\psi: W \to F \mid \phi(Tv)=0 \textrm{ for all } v \in V, T'(\psi)(v)=0, T'\psi = 0, \phi \in \nul(T')\}$. 
	\\ \textbf{Cor. } If $T: V \to W$ is a linear map between finite-dimensional $F$-vector spaces, then $\dim \nul(T') = \dim \nul(T) + \dim W - \dim V$. 
	\\ \textbf{Cor. } The linear map $T$ is onto iff $T'$ is 1-1. 
	\\ \textbf{Cor. } If $T: V \to W$ is a linear map between finite-dimensional vector spaces, then $T'$ and $T$ have equal ranks. 
	\\ \textbf{Cor. } We have $\textrm{range}T = (\nul T)^0$. 
	\\ \textbf{Theorem. } Let $T: V \to V$, $V$ finite-dimensional, and let $\alpha: F[x] \to \mathscr{L}(V)$, with $f \mapsto f(T)$. Also, we have $\ker\alpha$ to be the principal ideal $(m(x))$. Then, $m(x)$ is the minimal polynomial of $T$ and has degree $\leq n^2$. 
	\\ \textbf{Cor. } Let $\lambda_1,\dots,\lambda_t$ be distinct eigenvalues and take $E_i = E(\lambda_i,T) = \{v \in V \mid Tv = \lambda_iv\} \subseteq V$. Now, take $E_1 \times \dots \times E_t$. Then there exists a summation map $E_1 \times \dots \times E_t \xrightarrow[]{\textrm{sum}} V$ with $(v_1,\dots,v_t) \mapsto v_1 + \dots + v_t$. Then, the sum map is 1-1. 
	\\ \textbf{Cor. } Suppose $V$ is finite-dimensional. Then each operator on $V$ has at most $\dim V$ distinct eigenvalues. 
	\\ \textbf{Prop. } Suppose $T$ is an operator on an $F$-vector space $V$. If $f \in F[x]$ is a polynomial satisfied by $T$ (meaning $f(T)=0$), then every eigenvalue of $T$ on $V$ is a root of $f$. 
	\\ \textbf{Cor. } Suppose $\lambda$ is an eigenvalue of operator $T$ on a finite-dimensional $F$-vector space. Then $\lambda$ is a root of the minimal polynomial of $T$ iff $\lambda$ is an eigenvalue of $T$. 
	\\ \textbf{Prop. } Assume that $F = \mathbb{R}$ and that $f(x) := x^2 + bx + c$ is an irreducible polynomial. If $T \in \mathscr{L}(V)$ and $V$ is finite-dimensional, then the null space of $f(T)$ is even-dimensional. 
	\\ \textbf{Prop (honors version). } Let $T$ be an operator on a finite-dimensional vector space over $F$. If $p$ is an irreducible polynomial over $F$, then the dimension of the null space of $p(T)$ is a multiple of the degree of $p$. 
	\\ \textbf{Cor. } Every operator on an odd-dimensional $\mathbb{R}$-vector space has an eigenvalue. 
	\\ \textbf{Prop. } If $T$ is an operator on a finite-dimensional $F$-vector space, then the minimal polynomial of $T$ has degree at most $\dim V$. 
	\\ \textbf{Prop. } If $T$ is upper-triangular with respect to some basis of $V$, and if the diagonal entries of an upper-triangular matrix representation of $T$ are $\lambda_1,\dots,\lambda_n$, then $(T-\lambda_1I) \cdot \dots \cdot (T-\lambda_nI)=0$. 
	\\ \textbf{Prop. } Let $V$ be a finite-dimensional vector space and $T \in \mathscr{L}(V)$ and let $\lambda_1,\dots,\lambda_m$ be the eigenvalues of $T$. Then, $V = \oplus E(\lambda_i, T)$ iff $T$ is diagonalizable. 
	\\ \textbf{Prop. } TFAE. 
	\begin{enumerate}
		\\ $T$ is diagonalizable. 
		\\ $V$ has a basis consisting of eigenvectors. 
		\\ The direct sum $\underset{i}{\oplus} V_{\lambda_i}$ is all of $V$. 
		\\ $\dim \left(\underset{i}{\oplus} V_{\lambda_i} \right) = \dim V$. 
	\end{enumerate}
	\\ \textbf{Prop. } If $T: V \to V$ has $\dim V$ different eigenvalues, then $T$ is diagonalizable. 
	\\ \textbf{Jordan Canonical Form. } $X$ can be written as a direct sum of Jordan blocks, where $\sum$ dim(block) = $\dim X$. 
	\\ \textbf{Lemma. } Let $X = \oplus \textrm{span}(U_iv)$ for $i \in \{0,\dots,k_1\}$. If $Z$ is a subspace of $X'$ that is $U'$-invariant, then $\ann(Z) =: Y$ is $U$-invariant. 
	\\ \textbf{Lemma. } Suppose $S$ and $T$ are commuting operators on $V$. If $\lambda$ is an eigenvalue for $T$ on $V$, then the eigenspace $E(\lambda, T)$ is $S$-invariant. 
	\\ \textbf{Theorem. } The diagonalize operatosr on the same finite-dimensional vector space are simulateneously diagonalizable iff they commute with each other. 
	\\ \textbf{Theorem. } Every pair of commuting operators on a finite-dimensional nonzero complex vector spcae has a common eigenvector. 
	\\ \textbf{Prop. } Two commuting operators on a finite-dimensional nonzero complex vector space can be simultaneously upper-triangularized. 
	\\ \textbf{Prop. } Let $\alpha: V \to V'$ with $v \mapsto \phi_v$, where $\phi_v: V \to F$ such that $\phi_v(x) = \langle x,v \rangle$. Then, $\alpha(\lambda v) = \overline{\lambda}(\alpha(v))$ for $\lambda \in F$. 
	\\ \textbf{Prop. } If $V$ is finite-dim then $\alpha: V \to V'$ is an invertible linear map of $\mathbb{R}$-vector spaces. It is an isomorphism of $F$-vector spaces if $F=\mathbb{R}$ and a congugate-linear bijection if $F = \mathbb{C}$. 
	\\ \textbf{Prop. } If $v=a_1v_1 + \dots + a_mv_m$ and $v_1,\dots,v_m$ orthogonal, then $a_k = \langle v,v_k \rangle$, $k=1,\dots,m$. If $v_1,\dots,v_m$ is orthonormal basis of $V$ then $v=\langle v,v_1 \rangle v_1 + \dots + \langle v,v_m \rangle v_m$. 
	\\ \textbf{Prop. } If $V$ is a finite-dim inner product space, then $V$ has an orthonormal basis. 
	\\ \textbf{Prop. } Suppose $V$ is finite-dim. Then every orthonormal list of vectors in $V$ can be extended to an orthonormal basis of $V$. 
	\\ \textbf{Formula. } Assume $V$ is finite-dim and $U$ is a subspace of $V$. Then $\dim U^\perp = \dim V - \dim U$. 
	\\ \textbf{Prop. } Suppose $U$ is generated by a single nonzero vector $w$. Then $P_U(v) = \frac{\langle v,w \rangle}{\langle w,w \rangle}w$. 
	\\ \textbf{Prop. } If $e_1,\dots,e_d$ is an orthonormal basis of $U$< then $P_U(v) = \langle v,e_1 \rangle e_1 + \dots + \langle ,e_d \rangle e_d$. 
	\\ \textbf{Formula. } $\alpha_V T^* = T' \circ \alpha_W$, where $T: V \to W$ and $T^*: W \to V$. 
	\\ \textbf{Formula. } Let $T: V \to W$. Then $(T'\alpha_W(w))(v) = \langle Tv,w \rangle$. 
	\\ \textbf{Lemma. } If $T: V \to W$ is a linear map betwen finite-dim inner product spaces, then if $a \in F$, then $(aT)^* = \overline{a}T^*$. 
	\\ \textbf{Prop. } The matrix of $T^*$ is the conjugate transpose of the matrix of $T$ if the same orthonormal bases of $V$ and $W$ are used to compute the matrices. 
	\\ \textbf{Formula. } $\overline{a_{ij}} = \langle T^*w_i, v_j \rangle_V$ iff $a_{-j} = \langle v_j, T^*w_i \rangle_V$. 
	\\ \textbf{Theorem. } If $T$ is symmetric, then $T$ is orthonormal diagonalizable. 
	\\ \textbf{Theorem. } Every eigenvalue of a self-adjoint operator is real. 
	\\ \textbf{Cor. } If $T$ is an operator on a complex inner product space, then $\langle Tv,v \rangle \in \mathbb{R}$ for all $v \in V$ iff $T$ is self-adjoint. 
	\\ \textbf{Prop. } Alternating implies anti-symmetric. 
	\\ \textbf{Prop. } Let $x^2 + bx + c$ be an irreducible quadratic over $\mathbb{R}$. Then the operator $T^2 + bT + cI$ is injective on $V$. 
	\\ \textbf{Theorem. } An operator $T$ is normal iff $||Tv|| = ||T^*v||$ for all $v \in V$. 
	\\ \textbf{Prop. } If $T$ is normal and $Tv = \lambda v$, then $T^*v = \overline{\lambda}v$. 
	\\ \textbf{Prop. } Suppose $T$ is normal and $v,w$ eigenvectors for $T$ with different eigenvalues. Then the vectors $v,w$ are orthogonal. 
	\\ \textbf{Theorem. } If $T$ is normal and $F = \mathbb{C}$, then $T$ is diagonal in an orthonormal basis of $V$. 
	\\ \textbf{Prop. } Let $T: V \to V$ be a symmetric (self-adjoint) operator on a nonzero finite-dim inner product space. Then $T$ has an eigenvalue. 
	\\ \textbf{Prop. } If $T$ is self-adjoint, then it is diagonalizable in the real and complex case. 
	\\ \textbf{Prop. } Nilpotent $2x2$ operators (nonzero) have no square root. 
	\\ \textbf{Prop. } The operator $S$ is an isometry iff $V$ has an orthonormal basis of eigenvectors for which the corresponding eigenvalues have aboslute value 1. 
	\\ \textbf{Theorem. } If $T \in L(V)$, there is an isometry $S \in L(V)$ so that $T = S \sqrt{T^*T} = \textrm{(isometry)} \cdot \textrm{(positive operator)}$. 
	\\ \textbf{Observation. } 
	\begin{enumerate}
		\\ $T^*T$ is a positive operator. 
		\\ $\nul(T^*T) = \nul T$. 
		\\ $\range(T^*T) = \range(T^*)$. 
		\\ $\dim\range T = \dim\range T^*$. 
	\end{enumerate}
	\\ \textbf{Properties. } Let $A,B$ be $n \times n$ matrices. Then 
	\begin{enumerate}
		\\ $\det(AB) = \det A \cdot \det B$. 
		\\ $\det A  = 0$ iff $A$ is not invertible. 
		\\ If $m \in S_n$, then $\det \begin{pmatrix} A_{m_1} & \dots & A_{m_n} \end{pmatrix} = \sgn(m) \det A$. 
		\\ $\det \begin{pmatrix} A_1 & \dots & \alpha \cdot A_k & \dots & A_n \end{pmatrix} = \alpha\det A$. 
	\end{enumerate}
	\\ \textbf{Prop. } Let $S: V \to W$ be a linear map between inner product spaces. Then $S$ is an isometry iff all singular values of $S$ are 1. 	
	\\ \textbf{Theorem. } If the positive singular values of $T: V \to W$ are $s_1,\dots,s_m$, then there are orthogonal lists $e_1,\dots,e_m \in V$ and $f_1,\dots,f_m \in W$ so that $Tv = s_1\langle v,e_1 \rangle f_1 + \dots + s_m \langle v,e_m \rangle f_m$ for all $v \in V$. 
	\\ \textbf{Prop. } Let $T: V \to W$ be a linear map. If $s_1$ is the largest singular vlaue of $T$, then $||Tv|| \leq s_1 ||v||$ for all $v \in V$. 
	\\ HW SOLNS. 
	\begin{enumerate}
\item 1C. 
	\item 13 | prove that union of three subspaces of $V$ is a subspace iff one contains the other two | suffices to show if $W$ is the union of three of its subspaces, then then one of the three subspaces is contained in the union of the other two, by applying result of prob 1c.12. 
	\item 2C. 
	\item 8 | let $v_1,\dots,v_m$ be linearly independent in $V$ and $w \in V$. show $\dim span(v_1+w,\dots,v_m+w) \geq m-1$. | look at $v_1-v_2,\dots,v_1-v_m$ and it is contained in the dimspan. 
	\item 3A. 
	\item 11 | let $V$ be finitedim and $T \in L(V)$. show $T = \lambda I$ iff $ST=TS$ for all $S \in L(V)$. | for reverse direction, try contrapositive and look at $\ker T$. 
	\item 17 | let $V$ be finitedim. show the only two-sided ideals of $L(V)$ are $\{0\}$ and $L(V)$. | let $w$ be so that $Tw \neq 0$. let $S_k: V \to V$ that sends $v_j$ to 0 for $j \neq k$ and $v_k$ to $w$. put $R_k$ so that $R_k(Tw) = v_k$, and look at $R_kTS_kv_j$. 
	\item 3B. 
	\item 15 | Suppose there is a linear map on $V$ so that both null space and range of it are finitedim. show that $V$ is finite dim. | look at basis $Tv_1,\dots,Tv_n$ for range and $w_1,\dots,w_k$ for null space. 
	\item 19 | Let $W$ be finitedim and $T \in L(V,W)$. show $T$ is 1-1 iff there exists $S \in L(W,V)$ so that $ST=I$ on $V$. | letting $T: V \to W$ be 1-1 and looking at $U=\range T$, put $S: U \to V$ as the inverse of $T$ and extend to $S: W \to V$. 
	\item 20 | let $W$ be finite-dim and $T \in L(V,W)$. Show $T$ onto iff there exists $S \in L(W,V)$ so that $TS=I$ on $W$. | use ontoness of $T$ and look at restriction of $T$ to $X$, the complement of $null T$. do isomorphism $X \cong W$ and put $S: W \to X$ so that $TS=I$. 
	\item 3C. 
	\item 5 | Let $V,W$ be finitedim and $T \in L(V,W)$. show there is a basis of $V$ and a basis of $W$ so that in these bases, all entries of $M(T)$ are 0 except those in entries row $k$ col $k$ if $1 \leq k \leq \range T$. | $U=\nul T$ and $X$ is complement to $U$ in $V$. put bases of $X$ and $U$. find bases of $\range T$ and complete to get basis of $W$. 
	\item 6 | Let $v_1,\dots,v_n$ be basis of $V$ and $W$ is finitedim and let $T \in L(V)$. show there is a basis $w_1,\dots,w_m$ so that all entries of $M(T)$, in these bases, are 0 except possibly a 1 in the first row, first col. | first column is $Tv_1$. consider when $Tv_1 = 0, \neq 0$. put basis $W = span(Tv_1,w_2,\dots,w_m)$. 
	\item 7 | Let $w_1,\dots,w_n$ a basis of $W$ and $V$ finitedim and $T \in L(V,W)$. Show there is a basis $v_1,\dots,v_m$ of $V$ so that all entries in first row of $M(T)$, in these bases, are 0 except possilby a 1 in first row, first col. | Look at $T': W' \to V'$ and apply 3c.6 result. 
	\item 3D. 
	\item 10 | Let $V,W$ be finite dim and $U \subseteq V$. put $E = \{T \in L(V,W) \mid U \subseteq \nul T\}$. find a formula for $\dim E$ in terms of $\dim V, \dim U, \dim W$. | put $\Phi: L(V,W) \to L(U,W)$ by $\phi(T) = T \mid_U$ and find range and null space. 
	\item 19 | let $V$ be finitedim and $T \in L(V)$. show $T$ has same matrix with respect to every basis of $V$ iff $T = \lambda I$. | fix a matrix of $T$ and for basis $v_1,\dots,v_m$ of $V$, $v_1,\dots,(1/2)v_k,\dots,v_m$ is also basis; scale and edit. 
	\item 3E. 
	\item 9 | Show a nonempty subset $A$ of $V$ is a translate of some subspace of $V$ iff $\lambda v + (1-\lambda)w \in A$ for all $v,w \in A$, $\lambda \in F$. | for converse, fix $x \in A$ attempt for $A = x + U$, where $U = \{a-x \mid a \in A\}$. 
	\item 3F. 
	\item 6 | let $\phi,\beta \in V'$. show $\nul\phi \subseteq \nul\beta$ iff there is $c \in F$ so that $\beta = c\phi$. | by a previous problem, there is $S \in L(F)$ so that $\beta = S\phi$. 
	\item 26 | let $V$ be finitedim and $\Omega$ be a subspace of $V'$. show $\Omega = \{v \in V \mid \phi(v)=0 \forall \phi \in \Omega\}^0 = U^0$. | show $U = \cap_{i=1}^{m} (\nul \phi_i)$. 
	\item 5A. 
	\item 28 | let $V$ be finitedim and $T \in L(V)$. show $T$ has at most $1 + \dim\range T$ distinct eigenvalues. | put distinct eigenvalues/vectors and for nonzero eigenvalues, look at $v_i = T((1\lambda_i)v_i)$ and linear independence and range. 
	\item 39 | Let $V$ be finitedim and $T \in L(V)$. show $T$ has eigenvalue iff there is a subspace of $V$ of $\dim V - 1$ that is $T$-invariant. | one direction: use fact eigenvalues of $T_{V/U}$ are eigenvalues of $T$. other direction: if $\lambda$ eigenvalue, then $T-\lambda I$ noninvertible so its range has dim $< \dim V$. if $X = \range T$, every subspace $W$ of $V$ with $X \subseteq W \subseteq V$ is $T$-invariant. 
	\item 5B. 
	\item 2 | let $V$ be a complex vector space and $T \in L(V)$ have no eigenvalues. show every subspcae of $V$ invariant under $T$ is $\{0\}$ or infinite-dim. | Take instead a finitedim $X \subseteq V$; it has an eigenvector. 
	\item 4 | let $F=C$, $T \in L(V)$, $p \in P(C)$ is a nonconstant polynomial and $\alpha \in C$. show $\alpha$ is eigenvalue of $p(T)$ iff $\alpha = p(\lambda)$ for some eigenvalue $\lambda$ of $T$ | one direction: $p(T)v = p(\lambda)v$. other direction: $T$ is upper-triangular in some basis of $V$; look at diagonal and look at $p(T)$. 
	\item 5 | for above question, find an example where $V = R^2$ | take $\begin{pmatrix} 0 & -1 \\ 1 & 0 \end{pmatrix}$. 
	\item 7 | show if $V$ finitedim and $S,T \in L(V)$, then if at least one of $S,T$ inveritible, then minimal poly of $ST$ equals that of $TS$. | first show $Sp(T)S^{-1} = p(STS^{-1})$. then $T,STS^{-1}$ have same minimal poly. replace $T$ by $TS$. 
	\item 10 | let $V$ be finitedim and $T \in L(V)$. show $span(v,Tv,\dots,T^m v)=span(v,Tv,\dots,T^{\dim V - 1}v)$ for all $m \geq \dim V - 1$. | note $v,Tv,\dots,T^{m}v$ has dim $m$. 
	\item 19 | let $V$ be finitedim and $T \in L(V)$. let $\epsilon = \{q(T) \mid q \in P(F)\}$. show $\dim \epsilon$ = degree of minimal poly of $T$ | observe $F[x] / (\nul \alpha)$, algebra. 
	\item 25 | $V$ finitedim, $T \in L(V)$, $U \subseteq V$ invariant under $T$. show minimal poly of $T$ is poly multiple of minimal poly of $T_{V/U}$. also show (min poly of $T \mid_U$) x (min poly of $T_{V/U}$) is poly multiple of min poly of $T$. | first part: if $m$ is min poly of $T$, then $m(T \mid_U)$ is poly multiple of min poly of $T \mid_U$, similary for $T_{V/U}$. second part: let $g$ be min poly of $T_{V/U}$ and $f$ be min poly of $T \mid_U$ and show $(fg)(T)=0$. $g(T)$ is 0 map on $V/U$ and $f(T)$ maps $U$ to $\{0\}$. 
	\item 5C. 
	\item 7 | $V$ finitedim, $T \in L(V)$, and $v \in V$. show there is unique monic poly $p_v$ of smallest degree so that $p_v(T)v=0$. also show min poly of $T$ is a poly mult of $p_v$. | first part: $I = \{f(x) \mid f(T)v=0\}$, it contains 0 and closed under addition, and 'external multiplication' and use well-ordering. 
	\item 5D. 
	\item 2 | let $T \in L(V)$ have diagonal matrix $A$ corresponding to some basis of $V$. show that if $\lambda \in F$, then $\lambda$ appears on diag of $A$ exactly $\dim E(\lambda,T)$ times. | $E(\lambda,T) = \nul (T-\lambda I)$ and look at matrix multiplication. 
	\item 3 | $V$ finitedim, $T \in L(V)$ diagonalizable. show $V = \nul T \oplus \range T$. | look at eigenvalues that are 0 and eigenvalues that are nonzero. 
	\item 5 | $V$ finitedim complex vector space, $T \in L(V)$ and $V = \nul (T - \lambda I) \oplus \range (T - \lambda I)$ for all $\lambda \in C$. show $T$ diagonalizable. | do induction on $\dim V$. 
	\item 19 | prove/disprove: if $T \in L(V)$ and $U \subseteq V$ is invariant under $T$ so that $T \mid_U$ and $T_{V/U}$ are diagonalizable, then $T$ diagonalizable. | false: take $\begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix}$. 
	\item 5E. 
	\item 2 | let $\epsilon$ be subset of $V$ where every $T \in \epsilon$ is diagonalizable. show there is a basis of $V$ with respect to which every $T \in \epsilon$ has diag matrix iff every pair $S,T \in \epsilon$ commutes. | converse: look at direct sum of operators, eigenspaces, and restrictions. 
	\item 6 | $V$ finitedim nonzero complex vector space and $ST=TS$. show there exist $\alpha,\lambda \in C$ so that $\range (S-\alpha I) + \range (T-\lambda I) \neq V$. | look at 2 upper triangular matrices, one with $\alpha$ in bottom left corner and another with $\lambda$ in bottom left corner. 
	\item 10 | want commuting operators $S,T$ so that $S+T$ has an eigenvalue that is not sum of eigenvalue of $S$ and eigenvalue of $T$, and similarly for $ST$. | let $S = \begin{pmatrix} 0 & -1 \item 1 & 0 \end{pmatrix}, T = -S$. 
	\item 6A. 
	\item 1 | prove/disprove: if $v_1,\dots,v_m \in V$ then $\sum_{j=1}^{m} \sum_{k=1}^{m} \langle v_j,v_k \rangle \geq 0$. | true, do induction and apply formula for $||v_1 + \dots + v_m||^2$. 
	\item 4 | let $T \in L(V)$ so that $||Tv|| \leq ||v|| \forall v \in V$. show $T-\sqrt{2}I$ is injective. | do by contradiction and use triangle inequality. 
	\item 6B. 
	\item 1 | let $e_1,\dots,e_m \in V$ so that $||a_1e_1 + \dots + a_me_m||^2 = |a_1|^2 + \dots + |a_m|^2$. show $e_1,\dots,e_m$ is orthonormal | to show orthogonal, we have $||e_a||^2 \leq 1 + |a|^2 = ||e_a + ae_b||^2$. 
	\item 3 | let $e_1,\dots,e_m$ be orthonormal in $V \ni v$. show $||v||^2 = |\langle v,e_1, \rangle|^2 + \dots + |\langle v,e_m \rangle|^2$ iff $v \in span(e_1,\dots,e_m)$. | for forward direction, set $x = \langle v,e_1 \rangle e_1 + \dots + \langle v,e_m \rangle e_m$ - look at $\langle x,v \rangle$ and $||x-v||^2$. 
	\item 6 | let $e_1,\dots,e_n$ be an ON basis of $V$. (1) show if $v_1,\dots,v_n \in V$ so that $||e_i - v_i || \leq \frac{1}{\sqrt{n}}$, then the list of $v_i's$ is a basis of $V$. (2) show there are $v_1,\dots,v_n \in V$ so that $||e_i - v_i|| \leq \frac{1}{\sqrt{n}}$ but $v_i$'s are L.D. | (1): show linear independence and observe $|a_1|^2 + \dots + |a_n|^2 = ||a_1e_1 + \dots + a_ne_n||^2 = ||(a_1e_1 + \dots + a_ne_n) - (a_1v_1 + \dots + a_nv_n)||^2$, apply triangle, C-S inequalities. (2): put $v_i := e_i - \frac{1}{n}(e_1 + \dots + e_n)$. 
	\item 9 | let $e_1,\dots,e_m$ be the result of applying GPS to L.I. list $v_1,\dots,v_n \in V$. show $\langle v_k,e_k \rangle > 0 \forall k$. | for case when $v_1,\dots,v_n$ not orthogonal, show contrapositive and note $||v_a||^2 = |\langle v,e_1 \rangle|^2 + \dots + |\langle v_a,e_m \rangle|^2$. 
	\item 17 | let $F=C$ and $V$ finitedim. show if $T$ is an operator on $V$ so that 1 is only eigenvalue of $T$ and $||Tv|| \leq ||v|| \forall v \in V$, then $T=I$. | use schur's theorem; then diagonal entries are all 1. then write $Te_k$ as a linear combo of the $e_i$'s via matrix entries, upper bound coefficients to 0, so coefficients are 0, so $T=I$. 
	\item 7A. 
	\item 5 | let $T \in L(V,W)$. let $e_1,\dots,e_n$ be ON basis of $V$ and $f_1,\dots,f_m$ be ON basis of $W$. show $||Te_1||^2 + \dots + ||Te_n||^2 = ||T^*f_1||^2 + \dots + ||T^*f_m||^2$. | note $\sum ||Te_i||^2 = \sum \sum |\langle Te_i,f_j \rangle|^2$ and use inner product properties. 
	\item 29 | prove/disprove: if $T \in L(V)$, there is an ON basis $e_1,\dots,e_n$ so that $||Te_i|| = ||T^*e_i|| \forall i$. | false: take $T = \begin{pmatrix} 1 & 1 \\ -1 & 0 \end{pmatrix}$. 
	\item 7B. 
	\item 5 | prove/disprove: if $T \in L(C^3)$ is diagonalizable, then $T$ normal | false: take $v_1 = (1,0,0), v_2 = (0,1,0), v_3 = (1,0,1)$ and put $Tv_1 = v_1, Tv_2 = v_2, Tv_3 = 3v_3$. 
	\item 6 | $V$ complex inner product space and $T \in L(V)$ normal and $T^9=T^8$. show $T$ self-adjoint and $T^2=T$. | look at orthonormal basis of $V$ of eigenvectors and see eigenvalues in $\{0,1\}$. then by prev problem, $T=P_U$ for some $U \subseteq V$. 
	\item 8 | $F=C$, $T \in L(V)$, show $T$ normal iff each eigenvector of $T$ is eigenvector of $T^*$. | reverse direction: by class, $\exists$ ON basis of $V$ where $T$ is upper-triangular, observe matrices, apply complex spectral theorem. 
	\item 18 | $V$ inner product space. want $T \in L(V)$ so that $T^2 + bT + cI$ noninvertible with $b^2 < 4c$. | take $T = \begin{pmatrix} 0 & -1 \\ 1 & 0 \end{pmatrix}$. 
	\item 7C. 
	\item 5 | let $T \in L(V)$ self-adjoint. show $T$ positive iff for every ON basis $e_1,\dots,e_n$ of $V$, all entries on diagonal of $M(T,(e_1,\dots,e_n))$ are nonnegative. | forward: use thm 'writing a vector as a linear combo of ON basis' reverse: spectral theorem and equivalent statement to $T$ positive. 
	\item 7 | $S \in L(V)$ invertible \& positive and $T \in L(V)$ positive. show $S+T$ invertible | first show $X$ positive \& invertible $\iff$ $\rangle Xv,v \rangle \forall v \in V \setminus \{0\}$, then apply. 
	\item 15 | $T \in L(V)$ self-adjoint. show $\exists A,B \in L(V)$ so that $T=A-B, \sqrt{T^*T} = A+B, AB=BA=0$. | spectral theorem, and only real eigenvalues $\lambda_1,\dots,\lambda_n$. put $\alpha_i = \lambda_i$ if $\lambda_i \geq 0$, else, 0. put $\beta_i = -\lambda_i$ if $\lambda_i \leq 0$, else, 0. put $Ae_k = \alpha_ke_k$, $B$ similarly. 
	\item 18 | $S,T \in L(V)$, both positive. show $ST$ positive $\iff$ $ST=TS$. | forward: prf by contradiction gives $ST \neq (ST)^*$, so $ST$ not self-adjoint, contradiction reverse: there is ON basis $e_1,\dots,e_n$ of eigenvectors of $S,T$, so $Se_i = \mu_ie_i$, $Te_i = \lambda_ie_i$ with $\lambda_i,\mu_i \geq 0 \forall i$. 
	\item 7D. 
	\item 1 | $\dim V \geq 2$ and $S \in L(V,W)$. show $S$ isometry iff $Se_1,Se_2$ ON list in $W$ for all ON list $e_1,e_2$ in $V$. | forward: put $U := span(e_1,e_2)$ and look $S \mid_U$ \& apply equivalence thm from axler. reverse: fix ON basis of $V$ and look at equivalence thm from axler. 
	\item 2 | $T \in L(V,W)$. show $T = \lambda I$ iff $T$ preserves orthogonality. | reverse: fix ON basis of $V$, look at $\langle u+v,u-v \rangle = ||u||^2 - ||v||^2$ and apply to pairs in ON basis, put $\lambda := ||Te_i||$ \& do cases, $\lambda=0,\neq 0$. 
	\item 4 | $F=C$ and $A,B$ self-adjoint. show $A+iB$ unitary iff $AB=BA$, $A^2 + B^2 = I$. | forward: look at $||(A+ib)v||^2$ and $SS^*=I$ and inner products. 
	\item 7E. 
	\item 2 | let $T \in L(V,W)$ and $s>0$. show $s$ is singular value of $T$ iff $\exists$ nonzero $v \in V, w \in W$ so that $Tv=w, T^*w=v$. | forward; $e_1,\dots,e_m$ and $f_1,\dots,f_m$ ON lists of $V,W$ so that $Te_k = s_kf_k$, $T^*f_k = s_ke_k$. 
	\item 3 | give example of $T \in L(C^3)$ so that 0 is only eigenvalue of $T$ and singular values of $T$ are $0,5$. | Take $T = \begin{pmatrix} 0 & 5 \\ 0 & 0 \end{pmatrix}$. 
	\item 4 | $T \in L(V,W)$, $s_1$ is largest singular value of $T$, $s_n$ is smallest. show $[s_n,s_1] = \{||Tv|| \mid v \in V, ||v||=1\}$. | by cases. for case $s_1>s_n$, use bessel's inequality. 
	\item 9 | $T \in L(V,W)$. show $T,T^*$ have same positive eigenvalues | get ON lists $f_1,\dots,f_m \in W$, $e_1,\dots,e_m \in V$ by SVD and get $T^*w = s_1 \langle w,f_1 \rangle e_1 + \dots + s_m \langle w,f_m \rangle e_m$. 
	\item 11 | $T \in L(V,W)$, $v_1,\dots,v_n$ ON basis of $V$. put $s_1,\dots,s_n$ singular values of $T$. (1): show $||Tv_1||^2 + \dots + ||Tv_n||^2 = s_1^2 + \dots + s_n^2$. (2): if $W=V$ and $T$ positive, show $\langle Tv_1,v_1 \rangle + \dots + \langle Tv_n,v_n \rangle$. | (1): look at ON basis of $V$, and of $W$ and $Te_k = s_kf_k$. (2): $\sum_{i=1}^{n} \langle Tv_i,v_i \rangle = \sum_{i=1}^{n} ||\sqrt{T}v_i||^2 = s_1 + \dots + s_n$. 
	\item 15 | $T \in L(V)$ and $s_1 \geq \dots \geq s_n$ singuluar values. show if $\lambda$ eigenvalue of $T$, then $s_1 \geq |\lambda| \geq s_n$. | take $v \in V$ so $Tv = \lambda v, ||v||=1$, apply prev. problem result to get $|\lambda| = ||\lambda v|| = ||Tv|| \in [s_n,s_1]$. 
\end{enumerate}

\end{spacing}
\end{multicols}

}

\end{document}



