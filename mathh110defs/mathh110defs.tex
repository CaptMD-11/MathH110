\input{~/normal-preamble.tex}

\begin{center}
    Math H110 Definitions. 
\end{center}

\begin{enumerate}
    \item \textbf{Endomorphism. } An endomorphism is a group homomorphism from a set to itself (NOTE: does not have to be invertible.)
    \item \textbf{End V. } The symbol $\End V$ is the set of all endomorphisms on $V$ (and multiplication on $\End V$ is defined to be function composition). 
    \item \textbf{F-Module. } An $F-$module is a generalization of vector spaces over rings. 
    \item \textbf{Subspace. } Let $V$ be a vector space. $X$ is a subspace of $V$ if $X \subseteq V$ and closed under all relevant operations of $V$, $X \neq \emptyset$, and $X \ni 0$. 
    \item \textbf{Linear Map / Linear Transformation. } Let $V$ be a vector space over a field $F$ with $v,w \in V$. Let $T$ be a map on $V$ with $T(v+w) = T(v) + T(w)$ and $T(\lambda v) = \lambda T(v)$ for all $\lambda \in F$. Then, $T$ is called a linear map or linear transformation. 
    \item \textbf{Linear Operator. } If $T$ is a linear transformation on a vector spaces $V$ with $T: V \to V$, then $T$ is linear operator on $V$. 
    \item \textbf{Spans. } The list $v_1,\dots,v_n$ spans $V$ iff $T: F^n \to V$ is onto. 
    \item \textbf{Linearly Independent. } The list $v_1,\dots,v_n$ is linearly independent iff $T: F^n \to V$ is 1-1. Equivalently, the list $v_1,\dots,v_n$ is linearly independent if $\lambda_1v_1 + \dots + \lambda_nv_n = 0$ implies $\lambda_i=0$ for all $i$. 
    \item \textbf{Linearly Dependent. } The list $v_1,\dots,v_n$ is linearly dependent iff $\lambda_1v_1 + \dots + \lambda_nv_n = 0$ implies $\lambda_i \neq 0$ for some $i$. 
    \item \textbf{Basis. } The list $v_1,\dots,v_n$ is a basis of $V$ if $\textrm{span}\{v_1,\dots,v_n\} = V$ and $v_1,\dots,v_n$ is linearly independent. 
    \item \textbf{Finite-dimensional. } $V$ is finite-dimensional if $V$ is spanned by a finite list of vectors. 
    \item \textbf{Sum of Subspaces. } Let $X_1,\dots,X_t$ be subspaces of $V$. Then, we define their sum as $X_1 + \dots + X_t = \{x_1 + \dots + x_t \mid x_1 \in X_1, \dots, x_t \in X_t\}$. 
    \item \textbf{Direct Sum of Subspaces. } Let $X_1,\dots,X_t$ be subspaces of $V$. Then, their direct sum, $X_1 \oplus \dots \oplus X_t$, is given by a 1-1 linear map $T$, with $T: X_1\times \dots \times X_t \to V$.
	\item \textbf{Complement of Subspace. } Let $X,Y$ be subspaces of of $V$. Then, $Y$ is a complementary subspace of $X$ iff $X+Y=V$ and $X+Y = X \oplus Y$. 
	\item \textbf{Rank, Nullity. } The rank of a linear map is the dimension of the range of the linear map. The nullity is the dimension of the null space of the linear map. 
	\item \textbf{Null Space. } The null space is the set of vectors that are mapped to $0$. 
	\item \textbf{Isomorphic Vector Spaces. } Two vector spaces $V, W$ are isomorphic if there exists a linear map $T: V \to W$ that is 1-1 and onto. 
	\item \textbf{Quotient Space. } Suppose $U$ is a subspace of $V$. Then, the quotient space $V / U$ is the set $V / U = \{v + U \mid v \in V\}$. 
	\item \textbf{Column Rank. } The column rank (rank of the column span of a matrix) is defined to be $\textrm{rank}T_A$. 
	\item \textbf{Conjugation. } Let $A$ be an $n \times n$ matrix (over $F$) and let $Q$ be an $n \times n$ matrix (over $F$). Then, the conjugation of $A$ by $Q$ is $Q^{-1}AQ$.
	\begin{center}
		\hrule
	\end{center} 
	\item \textbf{Dual Space. } Let $V$ be an $F$-vector space. Then the dual space of $V$ is $V' = \mathscr{L}(V,F)$ where the elements of $V'$ are called linear functionals. 
	\item \textbf{Annihilator. } For a subspace $U \subseteq V$, we define the annihilator of $U$ to be $U_0 = \{\phi \in V' \mid \phi(u) = 0 \forall u \in U\}$. 
	\item \textbf{Double Dual. } Let $V$ be a finite-dimensional vector space with dual $V'$. Then the double dual of $V$ is $(V')' = V'' = V$. Also, $\dim V = n = \dim V' = \dim V''$. 
	\item \textbf{Eigenvector / eigenvalue. } Let $T \in \mathscr{L}(V)$. Then an eigenvector of $T$ is a $v \in V$ ($v \neq 0$) such that $Tv = \lambda v$ ($\lambda \in F$ is called an eigenvalue), and $v$ is an eigenvector of $T$. 
	\item \textbf{Eigenspace. } Let $T \in \mathscr{L}(V)$ and take $\lambda$ to be an eigenvalue of $T$. Then, $E(\lambda,T) = \{v \in V \mid Tv = \lambda v\} \neq \emptyset$ is written as $V_\lambda$ and is called the eigenspace of $\lambda$, which is a subspace of $V$. 
	\item \textbf{Invariant subspace. } $E$ is a $T$-invariant subspace if $T \in \mathscr{L}(V)$ with $T(E) \subseteq E$. 
	\item textbf{Idempotent. } If $e = e^2$, then $e$ is called idempotent. 
	\item \textbf{Generalized Eigenvector. } Consider a minimal polynomial $(x-\lambda_1)^{e_1} \cdot \dots \cdot (x-\lambda_m)^{e_m}$ on $X$ with $(T-\lambda_1I)^{e_1}v = 0$. Then, $v$ is called a generalized eigenvector for $\lambda = \lambda_1$. 
	\item \textbf{Characteristic polynomial. } The characteristic polynomial of $T: V \to V$ (with eigenvalues $\lambda_1,\dots,\lambda_t$) is the polynomial $\prod_{i=1}^{t} (x-\lambda_i)^{\dim X_i}$, where $V = X_1 \oplus \dots \oplus X_t$. 
	\item \textbf{Simultaneously diagonalizable. } Operators $S$ and $T$ on $V$ are simulatenously diagonalizable if there is a basis of $V$ that consts of vectors that are eigenvectors for both $S$ and $T$ (i.e. there exists a basis $v_1,\dots,v_n$ of $V$ so that for $i$, $1 \leq i \leq n$, there are $\lambda_i$ and $\mu_i$ so that $Sv_i = \lambda_iv_i$ and $Tv_i = \mu_iv_i$). 
	\begin{center}
		\hrule
	\end{center}
	\item \textbf{Inner product. } Let $V$ be a vector space over $\mathbb{R}$, possibly infinite-dim. An inner product on $V$ is a bilinear functon $\langle \cdot, \cdot, \rangle: V \times V \to \mathbb{R}$ such that $\langle x, x \rangle \geq 0$ for all $x \in V$ and $\langle x, x \rangle = 0$ iff $x =0$. 
	\item \textbf{Complex Inner product. } Let $z=(z_1,\dots,z_n)$ and $w=(w_1,\dots,w_n)$ be in $\mathbb{C}^n$. We have $\langle z, w \rangle = \sum_{i} z_i \overline{w_i}$, with $\langle w, z \rangle = \overline{\langle z, w \rangle}$ and $\langle \alpha z, w \rangle = \alpha \langle z, w \rangle$ and $\langle w, \alpha z \rangle = \overline{\alpha}\langle z, w \rangle$. 
	\item \textbf{Norm. } Norm of $v \in V$ is $||v|| = \sqrt{\langle v, v \rangle}$. 
	\item \textbf{Orthogonal. } Two vectors are orthogonal if $\langle v,w \rangle = 0$, where $v,w \in V$. 
	\item \textbf{Orthogonal Complement. } Take $U$ to be a subset of $V$. Then, $U^\perp = \{v \in V \mid \langle v,u \rangle = 0 \forall u \in U\} = \{v \in V \mid \langle u,v \rangle = 0 \forall u \in U\} = \{v \in V \mid \phi_v = 0 \textrm{ in } U\} = \{v \in V \mid \phi_v \in U^0\}$. 
	\item \textbf{Adjoint. } Let $T: V \to W$. Then the adjoint of $T$ is $T^*: W \to V$ such that $T^* = \alpha_V^{-1} \circ T' \circ \alpha_W$, where $\alpha_V: V \to V'$ and $\alpha_W: W \to W'$. 
	\item \textbf{Self-adjoint. } An operator $T: V \to V$ is self-adjoint if $T = T^*$. 
	\item \textbf{Symmetric. } $T$ is symmetric if $M(T) = M(T)^t$. 
	\item \textbf{Normal. } $T$ is a normal operator if $TT^* = T^*T$. 
	\item \textbf{Alternating. } A bilinear form $\psi: V \times V \to F$ is alternating if $psi(v,v) = 0$. for $v \in V$. 
	\item \textbf{Anti-symmetric. } Let $\psi: V \times V \to F$ be a bilinear form. Then $\psi$ is anti-symmetric if $\psi(x,y) = -\psi(y,x)$. 
	\item \textbf{Positive operator. } $T$ is a positive operator on an inner product space if $T=T^*$ and $\langle Tv,v \rangle \geq 0$. 
	\item \textbf{Square root of an operator. } A square root of an operator $T$ is an operator $R$ such that $R^2 = T$. 
	\item \textbf{Isometry. } An operator $S \in L(V)$ is an isometry if it preserves distance, i.e. $||Sv|| = ||v|| \forall v \in V$, i.e. $S^* = S^{-1}$. In the real case, they are called orthogonal operators. In the complex case, they are called unitary operators. 
	\item \textbf{Singular values. } The singular values of $T$ are the eigenvalues of $\sqrt{T^*T}$. 
	\item \textbf{Def 1 of Determinants. } (cofactor expansion). $\det A = \sum_{j=1}^n (-1)^{j+1} \cdot a_{ij} \det (A_{ij})$. 
	\item \textbf{Def 2 of Determinants. } If $T \in L(V)$, where $V$ is over $\mathbb{C}$, then the determinant of $T$ is the product of the eigenvalues $\lambda_1,\dots,\lambda_n$ (with multiplicity). 
	\item \textbf{Def 3 of Determinants. } The determinant of an operator $T \in L(V)$ is $(-1)^n$ times the constant term of its characteristic polynomial $(z-\lambda_1) \cdot \cdot \cdot (z-\lambda_n)$. 
	\item \textbf{Permutation. } A permutation $m=(m_1,\dots,m_r)$ is a list containing $1,\dots,n$ exactly once each, and write $S_n$ to denote the set of $n$-element permutations. 
	\item \textbf{Inversion. } An inversion (in a permutation) is a pair $\{i,j\}$ such that $m_i > m_j$ where $i < j$. 
	\item \textbf{Def 4 of Determinants. } Let $A$ be an $n \times n$ matrix. Then, $\det A = \sum_{m \in S_n} \left(\sgn(m) \cdot a_{m_1,1} \cdot \cdot \cdot a_{m_n,n}\right)$. 
	\item \textbf{Box. } The box defined by $v_1,\dots,v_n \in \mathbb{R}^n$ is $\{a_1v_1 + \dots + a_nv_n \mid 0 \leq a_i \leq 1, i =1,\dots,n\}$ and as volume, $\det \begin{pmatrix} v_1 & \dots & v_n \end{pmatrix}$, where $v_1,\dots,v_n$ are the columns of a matrix $A$. 
	\item \textbf{Norm of $T$. } This is defined to be $\sup_{v \in V \setminus \{0\}} \frac{||Tv||}{||v||}$. 
	\item \textbf{Tensor product. } $V \otimes W := \Bil(V',W')$. 
\end{enumerate}

\end{document}
