\input{~/normal-preamble.tex}

\begin{center}
    Math H110 Theorems. 
\end{center}

\begin{enumerate}
    \item \textbf{Lemma. } Let $F$ be a field, $\lambda \in F$, $V$ a vector space over $F$ (denoted by $V/F$), $v \in V$. Then, if $\lambda v = 0$, then $\lambda=0$ or $v=0$. 
    \item \textbf{Lemma. } A vector space over a field is a module over a field. 
    \item \textbf{Theorem. } The intersection of a family of subspaces of a vector space $V$ is a subspace of $V$. 
    \item \textbf{Lemma. } Let $S=\{v_1,\dots,v_t\}$. Then the subspace of all linear combinations of the elements of $S$ is the $\mathrm{span}S$. 
    \item \textbf{Theorem. } Let $L=v_1,\dots,v_n$ be a list of vectors in a vector space $V$ over a field $F$ and let $T: F^n: \to V$ be linear transformation with $(\lambda_1,\dots,\lambda_n) \mapsto \lambda_1v_1 + \dots + \lambda_nv_n$. Then, we have the following: 
    \begin{enumerate}
        \item $L$ spans $V$ iff $T$ is onto. 
        \item $L$ is linearly independent iff $T$ is 1-1 iff $\nul T = \{0\}$. 
        \item $L$ is a basis iff $T$ is 1-1 and onto. 
    \end{enumerate}
    \item \textbf{Prop. } Consider $T: F^n \to V$ with $(\lambda_1,\dots,\lambda_n) \mapsto \lambda_1v_1 + \dots + \lambda_nv_n$, so $T(e_i)=v_i$ for all $i$. Then, $T$ is the unique linear map $F_n \to V$ that sends $e_i \mapsto v_i$ for all $i$. 
    \item \textbf{Theorem. } Every subspace $X$ of $V$ has complement. 
	\item \textbf{Lemma. } If $v_1,\dots,v_t$ is linearly dependent list, then there is an index $k$ such that $v_k \in \textrm{span}(v_1,\dots,v_{k-1},v_{k+1},\dots,v_t)$. Furthermore, the span of the list of length $t-1$ gotten by removing $v_k$ from the list is the same as the span of the original list. 
	\item \textbf{Prop. } In a finite-dimensional vector space, the length is of every linearly independent list of vectors is less than or equal to the length of every spanning list of vectors. 
	\item \textbf{Cor. } Two bases of $V$ have the same number of elements. 
	\item \textbf{Prop. } $X+Y$ is direct iff the null space of the sum map is $\{0\}$. 
	\item \textbf{Theorem. } Every subspace of a finite-dimensional vector space is finite-dimensional. 
	\item \textbf{Prop. } Every spanning list for a vector space can be pruned down to a basis of the space. 
	\item \textbf{Cor. } Every finite-dimensional vector space has a basis. 
	\item \textbf{Prop. } In a finite-dimensional vector space, every linearly independent list can be extended to a basis of the space. 
	\item \textbf{Major Theorem. } Every subspace of a finite-dimensional vector space has a complement. 
	\item \textbf{Prop. } Let $X,Y$ be subspaces of a finite-dimensional vector space $V$. Then: 
	\begin{enumerate}
		\item $\dim X + \dim Y = \dim V$. 
		\item $X \cap Y = \{0\}$. 
	\end{enumerate}
	Then, $V=X \oplus Y$. 
	\item \textbf{Prop. } $\dim(X \oplus Y) = \dim X + \dim Y$. 
	\item \textbf{Prop. } If $V$ is a finite-dimensional vector space (with $\dim V = n$), then every subspace has dimension at most $n$. 
	\item \textbf{Prop. } Let $\dim V = n$. Then, a linearly independent list of vectors of $V$ with length $n$ is a basis for $V$. 
	\item \textbf{Prop. } Let $\dim V = n$. Then, every spanning list for $V$ of length $n$ is a basis for $V$.
	\item \textbf{Lemma. } The list $(x_1,0),\dots,(x_t,0);(0,y_1),\dots,(0,y_k)$ of length $t+k$ is a basis of $X \times Y$. 
	\item \textbf{Cor. } $\dim (X \times Y) = \dim X + \dim Y$. 
	\item \textbf{Cor. } Let $T: V \to W$ be a linear map with $\dim V = d$. Then, $\textrm{rank}T \leq d$. 
	\item \textbf{Rank-Nullity Theorem. } $\dim V = \textrm{rank}V + \textrm{nullity}V$. 
	\item \textbf{Prop. } If $T: V \to W$ is 1-1, then $\textrm{nullity}T = 0$. 
	\item \textbf{Cor. } If $T: V \to W$ is 1-1 and onto, then $\dim V = \dim W$. 
	\item \textbf{Theorem. } The set of linear maps $V \to W$ is a vector space $L \cdot (F^n,W) \to T \longrightarrow (Te_1,\dots,Te_n) \in W^n$. 
	\item \textbf{Theorem. } $\dim (X+Y) = \dim X + \dim Y - \dim (X \cap Y)$. 
	\item \textbf{Cor. } $\dim (V/X) = \dim V - \dim X$. 
	\item \textbf{Theorem. } If $A$ is a rectangular matrix with elements in a field $F$, then row rank $A$ = column rank $A$. 
	\item \textbf{Prop. } Let $T: V \to W$ be 1-1. Then, $\dim W \geq \dim V$. 
	\item \textbf{Prop. } Let $T: V \to W$ be onto. Then, $\dim V \geq \dim W$. 
	\item \textbf{Prop. } Let $T: V \to W$ and $\dim V = \dim W$. Then, $T$ 1-1 iff $T$ onto iff $T$ bijective iff $T$ invertible. 
	\begin{center}
		\hrule
	\end{center}
	\item \textbf{Lemma. } Let $V$ be a finite-dimensional vector space and $U$ a subspace of $V$. Then, $\dim U_0 = \dim V - \dim U$. 
	\item \textbf{Theorem. } Every linear functional on a subspace of $V$ can be extended to $V$. 
	\item \textbf{Note. } Annihilator is the dual of the quotient subspace. 
	\item \textbf{Theorem. } Let $T: V \to W$ and $T': W' \to V'$. THen $\mathscr{M}(T)$ and $\mathscr{M}(T')$ are transposes of each other. 
	\item \textbf{Lemma. } $U^0$ has dimension $\dim V - \dim U$. 
	\item \textbf{Cor. } The annihilator of $U$ is $\{0\}$ iff $U = V$. The annihilator of $U$ is $V$ iff $U = \{0\}$. 
	\item \textbf{Prop. } If $T: V \to W$ is a linear map, then the null space of $T'$ is the annihilator of the range of $T$. We have $\textrm{ann}(\textrm{range}T) = \{\psi: W \to F \mid \phi(Tv)=0 \textrm{ for all } v \in V, T'(\psi)(v)=0, T'\psi = 0, \phi \in \nul(T')\}$. 
	\item \textbf{Cor. } If $T: V \to W$ is a linear map between finite-dimensional $F$-vector spaces, then $\dim \nul(T') = \dim \nul(T) + \dim W - \dim V$. 
	\item \textbf{Cor. } The linear map $T$ is onto iff $T'$ is 1-1. 
	\item \textbf{Cor. } If $T: V \to W$ is a linear map between finite-dimensional vector spaces, then $T'$ and $T$ have equal ranks. 
	\item \textbf{Cor. } We have $\textrm{range}T = (\nul T)^0$. 
	\item \textbf{Theorem. } Let $F$ be a finite field with $q = |F|$. Then, $a^q=a$ for all $a \in F$. 
	\item \textbf{Theorem. } If $F$ is a finite field, then $|F|=p^n$ for some prime $p$ and integer $n \geq 1$. 
	\item \textbf{Theorem. } Take an ideal $I$ in $\mathbb{Z}$. Then, $I$ is equal to either $\{0\}$ or $m\mathbb{Z}$ (where $m \in \mathbb{Z}_{>0}$). 
	\item \textbf{Theorem. } $F[x]$ is a principal ideal domain; that is, it is an integral domain in which every ideal in $F[x]$ is principal. 
	\item \textbf{Theorem. } Let $T: V \to V$, $V$ finite-dimensional, and let $\alpha: F[x] \to \mathscr{L}(V)$, with $f \mapsto f(T)$. Also, we have $\ker\alpha$ to be the principal ideal $(m(x))$. Then, $m(x)$ is the minimal polynomial of $T$ and has degree $\leq n^2$. 
	\item \textbf{Cayley-Hamilton Theorem. } Let $T: V \to V$, $V$ finite-dimensional, and let $\alpha: F[x] \to \mathscr{L}(V)$, with $f \mapsto f(T)$. Also, we have $\ker\alpha$ to be the principal ideal $(m(x))$, where $m(x)$ is the minimal polynomial of $T$. Then, the characteristic polynomial is in $\ker\alpha$; that is, we can plug in the matrix for $T$ into its characteristic polynomial and we get that it is equal to the 0-matrix. 
	\item \textbf{Prop. } For $f(x) \in F[x]$ and $\lambda \in F$, $f(\lambda)=0$ iff $f$ is divisible by $x - \lambda$, where $x-\lambda$ is an irreducible polynomial. 
	\item \textbf{Cor. } A polynomial of degree $n$ can have at most $n$ roots. 	
	\item \textbf{Cor. } A polynomial with infinitely many roots is identically the zero polynomial. 
	\item \textbf{Lemma. } Let $f \in \mathbb{R}[x]$ be a real polynomial. If $\lambda$ is a complex root of $f$, so is $\overline{\lambda}$, which is the complex conjugate of $\lambda$. 
	\item \textbf{Prop. } A scalar $\lambda$ is an eigenvalue of $T: V \to V$ iff $T-\lambda I$ is not 1-1. 
	\item \textbf{Cor. } The map $T: V \to V$ is invertible iff 0 is not an eigenvalue of $T$. 
	\item \textbf{Key lemma. } Every list of eigenvectors of $T$ that corresponds to distinct eigenvalues of $T$ is a linearly independent list. 
	\item \textbf{Cor. } Let $\lambda_1,\dots,\lambda_t$ be distinct eigenvalues and take $E_i = E(\lambda_i,T) = \{v \in V \mid Tv = \lambda_iv\} \subseteq V$. Now, take $E_1 \times \dots \times E_t$. Then there exists a summation map $E_1 \times \dots \times E_t \xrightarrow[]{\textrm{sum}} V$ with $(v_1,\dots,v_t) \mapsto v_1 + \dots + v_t$. Then, the sum map is 1-1. 
	\item \textbf{Cor. } Suppose $V$ is finite-dimensional. Then each operator on $V$ has at most $\dim V$ distinct eigenvalues. 
	\item \textbf{Prop. } Suppose $T$ is an operator on an $F$-vector space $V$. If $f \in F[x]$ is a polynomial satisfied by $T$ (meaning $f(T)=0$), then every eigenvalue of $T$ on $V$ is a root of $f$. 
	\item \textbf{Cor. } Suppose $\lambda$ is an eigenvalue of operator $T$ on a finite-dimensional $F$-vector space. Then $\lambda$ is a root of the minimal polynomial of $T$. 
	\item \textbf{Prop. } Let $T$ be an operator on a finite-dimensinoal vector space. Suppose $\lambda$ is a root of the minimal polynomial. Then $\lambda$ is an eigenvalue of $T$. 
	\item \textbf{Theorem. } All operators on a nonzero finite-dimensional vector space over an algebraically closed field have at least one eigenvalue. 
	\item \textbf{Prop. } Assume that $F = \mathbb{R}$ and that $f(x) := x^2 + bx + c$ is an irreducible polynomial. If $T \in \mathscr{L}(V)$ and $V$ is finite-dimensional, then the null space of $f(T)$ is even-dimensional. 
	\item \textbf{Prop (honors version). } Let $T$ be an operator on a finite-dimensional vector space over $F$. If $p$ is an irreducible polynomial over $F$, then the dimension of the null space of $p(T)$ is a multiple of the degree of $p$. 
	\item \textbf{Prop. } $F[x] / (p)$ (where $p$ is irreducible) is a field. 
	\item \textbf{Formula. } $\dim_F V = [K:F] \cdot \dim_K V = \dim_F K \cdot \dim_K V$. 
	\item \textbf{Cor. } Every operator on an odd-dimensional $\mathbb{R}$-vector space has an eigenvalue. 
	\item \textbf{Prop. } If $T$ is an operator on a finite-dimensional $F$-vector space, then the minimal polynomial of $T$ has degree at most $\dim V$. 
	\item \textbf{Prop. } If $T$ is upper-triangular with respect to some basis of $V$, and if the diagonal entries of an upper-triangular matrix representation of $T$ are $\lambda_1,\dots,\lambda_n$, then $(T-\lambda_1I) \cdot \dots \cdot (T-\lambda_nI)=0$. 
	\item \textbf{Prop. } Let $V$ be a finite-dimensional vector space and $T \in \mathscr{L}(V)$ and let $\lambda_1,\dots,\lambda_m$ be the eigenvalues of $T$. Then, $V = \oplus E(\lambda_i, T)$ iff $T$ is diagonalizable. 
	\item \textbf{Prop. } TFAE. 
	\begin{enumerate}
		\item $T$ is diagonalizable. 
		\item $V$ has a basis consisting of eigenvectors. 
		\item The direct sum $\underset{i}{\oplus} V_{\lambda_i}$ is all of $V$. 
		\item $\dim \left(\underset{i}{\oplus} V_{\lambda_i} \right) = \dim V$. 
	\end{enumerate}
	\item \textbf{Prop. } If $T: V \to V$ has $\dim V$ different eigenvalues, then $T$ is diagonalizable. 
	\item \textbf{Prop. } The operator $T: V \to V$ is diagonalizable iff its minimal polynomial splits completely as a product of distinct linear factors of the form $x-r$. 
	\item \textbf{Jordan Canonical Form. } $X$ can be written as a direct sum of Jordan blocks, where $\sum$ dim(block) = $\dim X$. 
	\item \textbf{Lemma. } Let $X = \oplus \textrm{span}(U_iv)$ for $i \in \{0,\dots,k_1\}$. If $Z$ is a subspace of $X'$ that is $U'$-invariant, then $\ann(Z) =: Y$ is $U$-invariant. 
	\item \textbf{Lemma. } Suppose $S$ and $T$ are commuting operators on $V$. If $\lambda$ is an eigenvalue for $T$ on $V$, then the eigenspace $E(\lambda, T)$ is $S$-invariant. 
	\item \textbf{Theorem. } The diagonalize operatosr on the same finite-dimensional vector space are simulateneously diagonalizable iff they commute with each other. 
	\item \textbf{Theorem. } Every pair of commuting operators on a finite-dimensional nonzero complex vector spcae has a common eigenvector. 
	\item \textbf{Prop. } Two commuting operators on a finite-dimensional nonzero complex vector space can be simultaneously upper-triangularized. 
	\item \textbf{Prop. } We have: 
	\begin{enumerate}
		\item Every eigenvalue of $S+T$ is the sum of an eigenvalue of $S$ and an eigenvalue of $T$. 
		\item Every eigenvalue of $ST$ is the product of an eigenvalue of $S$ and an eigenvalue of $T$. 
	\end{enumerate}
	\begin{center}
		\hrule
	\end{center}
	\item \textbf{Formula. } $\langle v+w, v+w \rangle = \langle v,v \rangle + \langle w,w \rangle + 2\langle v,w \rangle$. 
	\item \textbf{Lemma. } If $u,v$ are orthogonal, then $||u+v||^2 = ||u||^2 + ||v||^2$. 
	\item \textbf{Lemma. } If $v \in V$ and $v \neq 0$, then every $u \in V$ is the sum of a multiple of $v$ and a vector orthogonal to $v$. 
	\item \textbf{Prop (Cauchy-Schwarz). } For $u,v \in V$, we have $|\langle u,v \rangle| \leq ||u|| \cdot ||v||$. 
	\item \textbf{Prop (Triangle inequality). } For $u,v \in V$, $||u+v|| \leq ||u|| + ||v||$. 
	\item \textbf{Prop. } The triangle inequality is equality iff one of $u,v$ is a nonnegative (real) multiple of the other. 
	\item \textbf{Prop. } Let $\alpha: V \to V'$ with $v \mapsto \phi_v$, where $\phi_v: V \to F$ such that $\phi_v(x) = \langle x,v \rangle$. Then, $\alpha(\lambda v) = \overline{\lambda}(\alpha(v))$ for $\lambda \in F$. 
	\item \textbf{Prop. } If $V$ is finite-dim then $\alpha: V \to V'$ is an invertible linear map of $\mathbb{R}$-vector spaces. It is an isomorphism of $F$-vector spaces if $F=\mathbb{R}$ and a congugate-linear bijection if $F = \mathbb{C}$. 
	\item \textbf{Riesz Representation Theorem. } Let $V$ be a finite-dim inner product space over $F$ (which is $\mathbb{R}$ or $\mathbb{C}$). If $\phi$ is a linear functional on $V$, there is a unique $v \in V$ such that $\phi(x) = \langle x,v \rangle$ for all $x \in V$. 
	\item \textbf{Prop. } An orthogonal list that consists of nonzero vectors is linearly independent. 
	\item \textbf{Cor. } An orthonormal list is linearly independent. 
	\item \textbf{Prop. } If $v_1,\dots,v_m$ is orthonormal and $a_1,\dots,a_m$ are elements of $F$, then $||a_1v_1 + \dots + a_mv_m||^2 = |a_1|^2 + \dots |a_m|^2$. 
	\item \textbf{Prop. } If $v=a_1v_1 + \dots + a_mv_m$ and $v_1,\dots,v_m$ orthogonal, then $a_k = \langle v,v_k \rangle$, $k=1,\dots,m$. If $v_1,\dots,v_m$ is orthonormal basis of $V$ then $v=\langle v,v_1 \rangle v_1 + \dots + \langle v,v_m \rangle v_m$. 
	\item \textbf{Prop. } If $V$ is a finite-dim inner product space, then $V$ has an orthonormal basis. 
	\item \textbf{Prop. } Suppose $V$ is finite-dim. Then every orthonormal list of vectors in $V$ can be extended to an orthonormal basis of $V$. 
	\item \textbf{Schur's Theorem. } Let $T$ be an operator on a finite-dim inner product space $V$. If $T$ is upper-triangular with respect to some basis, then it is upper-triangular with respect to some orthonormal basis of $V$. 
	\item \textbf{Prop. } If $U \subseteq W$, then $W^\perp \subseteq U^\perp$. 
	\item \textbf{Prop. } If $U$ is a subset of $V$, then $U \cap U^\perp \subseteq \{0\}$. 
	\item \textbf{Lemma. } If $U$ is a finite-dim subspace of $V$, then $V=U \oplus U^\perp$. 
	\item \textbf{Formula. } Assume $V$ is finite-dim and $U$ is a subspace of $V$. Then $\dim U^\perp = \dim V - \dim U$. 
	\item \textbf{Theorem. } If $U$ is a finite-dim subspace of $V$, then $(U^\perp)^\perp = U$. 
	\item \textbf{Prop. } Suppose $U$ is generated by a single nonzero vector $w$. Then $P_U(v) = \frac{\langle v,w \rangle}{\langle w,w \rangle}w$. 
	\item \textbf{Prop. } If $e_1,\dots,e_d$ is an orthonormal basis of $U$< then $P_U(v) = \langle v,e_1 \rangle e_1 + \dots + \langle ,e_d \rangle e_d$. 
	\item \textbf{Formula. } $\alpha_V T^* = T' \circ \alpha_W$, where $T: V \to W$ and $T^*: W \to V$. 
	\item \textbf{Formula. } Let $T: V \to W$. Then $(T'\alpha_W(w))(v) = \langle Tv,w \rangle$. 
	\item \textbf{Prop. } $\langle Tv,w \rangle_W = \langle v,T^*w \rangle_V$. 
	\item \textbf{Lemma. } If $T: V \to W$ is a linear map between finite-dim inner product spaces, then $(T^*)^* = T$. 
	\item \textbf{Lemma. } If $T: V \to W$ is a linear map betwen finite-dim inner product spaces, then if $a \in F$, then $(aT)^* = \overline{a}T^*$. 
	\item \textbf{Prop. } If $M(T) = (a_{ij})$, then $M(T^*) = (\overline{a_{ij}})^t$. 
	\item \textbf{Prop. } 
	\begin{enumerate}
		\item $I^* = I$. 
		\item $(S+T)^* = S^* + T^*$. 
		\item $(ST)^* = T^*S^*$. 
		\item $(T^{-1})^* = (T^*)^{-1}$. 
	\end{enumerate}
	\item \textbf{Prop. } The matrix of $T^*$ is the conjugate transpose of the matrix of $T$ if the same orthonormal bases of $V$ and $W$ are used to compute the matrices. 
	\item \textbf{Formula. } $\overline{a_{ij}} = \langle T^*w_i, v_j \rangle_V$ iff $a_{-j} = \langle v_j, T^*w_i \rangle_V$. 
	\item \textbf{Prop. } $\nul (T^*) = (\range T)^\perp$. 
	\item \textbf{Formula. } 
	\begin{enumerate}
		\item $\range (T^*) = (\nul T)^\perp$. 
		\item $\nul T = (\range(T^*))^\perp$. 
		\item $\range T = (\nul(T^*))^\perp$. 
	\end{enumerate}
\end{enumerate}

\end{document}
